{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CBOW Implementation of Word2Vec\n",
    "\n",
    "This is part of the background research that I'm working on for [viberary.pizza](https://viberary.pizza/).\n",
    "\n",
    "## Background \n",
    "\n",
    "[Word2vec](https://arxiv.org/abs/1301.3781) was a critical point in NLP work, building on previous work in dimensionality reduction in NLP such as tf-idf, topic modeling, and latent semantic analysis to reduce vocabulary sizes for computational complexity, and additionally, to add context by embedding similar words in the same latent space. \n",
    "\n",
    "As of 2022, it's almost been superceded by [transformers-based architectures](https://e2eml.school/transformers.html), but it's still worth understanding how it works in a historical context, as well as because there is a fair amount of it [in production in Spark](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.ml.feature.Word2Vec.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Implementation\n",
    "\n",
    "There are numerous word2vec implementations in libraries like Spark and Tensorflow. There is not an exact one in PyTorch, but following[this code](https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py), as well as reading about the [architecture here](https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0) and [here](https://jalammar.github.io/illustrated-word2vec/),  I was able to implement and understand how it works under te covers\n",
    "\n",
    "\n",
    "Original explanation in [PyTorch implementation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) is here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a starting point, we take our raw data for input. \n",
    "Our [training set for Viberary is some sample the Goodreads input dataset](https://github.com/veekaybee/viberary#input-data-sample), \n",
    "which is a string of text containing the metadata for each unique book\n",
    "id. \n",
    "\n",
    "For a single book id, it will contain the book description book title, etc. So a sample of a single book, will look like this\n",
    "\n",
    "\n",
    "```\n",
    "Raw text: All's Fairy in Love and War (Avalon: Web of Magic, #8) To Kara's astonishment, she discovers that a portal has opened in her bedroom closet and two goblins \n",
    "have fallen through! They refuse to return to the fairy realms and be drafted for an impending war. \n",
    "In an attempt to roust the pesky creatures, Kara falls through the portal, smack into the middle of a huge war.\n",
    "Kara meets Queen Selinda, who appoints Kara as a Fairy Princess and assigns her an impossible task: \n",
    "to put an end to the war using her diplomatic skills.\n",
    "```\n",
    "\n",
    "This is initially stored as a Python string. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final goal in learning a Word2Vec model with CBOW is, given an input phrase over a context window, to predict the word that's missing. The context window is how many words before and after the word we care about. So, given the phrase \"Kara falls X the portal\", we should be able to predict that the correct word is \"through.\"\n",
    "\n",
    "We do this in Word2Vec by continuously sampling from the raw text over the context window, where the context window around the word is the X variable and the word itself is the target variable. \n",
    "\n",
    "For the first example, \"Kara falls the portal\" is the context and \"through\" is the response variable. Then we shift the window by 1 word and generate another entry. This is the whole of the [continuous bag of words approach.](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "When we're first training the model, we pass these samples into the model and ask it to make a prediction on a single word given all these samples. The output is a vector of propabilities of the sample related to each word. We then compare that prediction to the actual label (I.e. for the sample \"Kara falls X the portal\" we KNOW the correct word is \"through\").\n",
    "\n",
    "We compare the actual vector (i.e. where through = 1) to the probability vector, and the difference between the two is the loss. The parameters are passed to the model across multiple epochs and continuously updated until we minimize the loss, i.e. we get as close to the predicted word as possible. \n",
    "\n",
    "In the process of doing this prediction, we create a lookup table of words, or embeddings matrix, to their vector representations. It is these vectors that become our embeddings. Andiamo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:23.985463Z",
     "iopub.status.busy": "2023-08-07T11:06:23.985135Z",
     "iopub.status.idle": "2023-08-07T11:06:25.147627Z",
     "shell.execute_reply": "2023-08-07T11:06:25.147111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch  # if you don't have it already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:25.150165Z",
     "iopub.status.busy": "2023-08-07T11:06:25.149891Z",
     "iopub.status.idle": "2023-08-07T11:06:25.881136Z",
     "shell.execute_reply": "2023-08-07T11:06:25.880877Z"
    }
   },
   "outputs": [],
   "source": [
    "# we need these bois\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:25.882736Z",
     "iopub.status.busy": "2023-08-07T11:06:25.882633Z",
     "iopub.status.idle": "2023-08-07T11:06:25.884242Z",
     "shell.execute_reply": "2023-08-07T11:06:25.884030Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, we'll initialize our hyperparameters for the model:\n",
    "\n",
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right - this is our context window\n",
    "EMBEDDING_DIM = 100  # size of the embeddings matrix - we'll get to this in a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:25.885384Z",
     "iopub.status.busy": "2023-08-07T11:06:25.885310Z",
     "iopub.status.idle": "2023-08-07T11:06:25.886752Z",
     "shell.execute_reply": "2023-08-07T11:06:25.886550Z"
    }
   },
   "outputs": [],
   "source": [
    "# Our tiny training dataset\n",
    "\n",
    "raw_text = \"\"\"To Kara's astonishment, she discovers that a portal has opened in her bedroom closet and two goblins have fallen through! They refuse to return to the fairy realms and be drafted for an impending war. \n",
    "In an attempt to roust the pesky creatures, Kara falls through the portal, \n",
    "smack into the middle of a huge war. Kara meets Queen Selinda, who appoints \n",
    "Kara as a Fairy Princess and assigns her an impossible task: \n",
    "to put an end to the war using her diplomatic skills.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:25.887817Z",
     "iopub.status.busy": "2023-08-07T11:06:25.887745Z",
     "iopub.status.idle": "2023-08-07T11:06:25.889156Z",
     "shell.execute_reply": "2023-08-07T11:06:25.888976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Text preprocessing get only individual words\n",
    "vocab = set(raw_text)  # dedup\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:25.890262Z",
     "iopub.status.busy": "2023-08-07T11:06:25.890184Z",
     "iopub.status.idle": "2023-08-07T11:06:25.891722Z",
     "shell.execute_reply": "2023-08-07T11:06:25.891513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'opened', 'fallen', 'refuse', 'impending', 'bedroom', 'portal', 'has', 'attempt', 'Queen', 'meets', 'smack', 'discovers', 'goblins', 'realms', 'return', 'war.', 'have', 'impossible', \"Kara's\", 'the', 'closet', 'Selinda,', 'creatures,', 'war', 'To', 'fairy', 'Kara', 'put', 'assigns', 'Princess', 'an', 'skills.', 'astonishment,', 'to', 'into', 'a', 'pesky', 'falls', 'for', 'two', 'middle', 'portal,', 'huge', 'Fairy', 'task:', 'using', 'roust', 'her', 'that', 'through!', 'drafted', 'and', 'she', 'They', 'in', 'be', 'of', 'who', 'as', 'through', 'diplomatic', 'appoints', 'In', 'end'}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:25.892794Z",
     "iopub.status.busy": "2023-08-07T11:06:25.892733Z",
     "iopub.status.idle": "2023-08-07T11:06:25.894312Z",
     "shell.execute_reply": "2023-08-07T11:06:25.894128Z"
    }
   },
   "outputs": [],
   "source": [
    "# we create simple mappings of word to an index of the word\n",
    "word_to_ix = {word: ix for ix, word in enumerate(vocab)}\n",
    "ix_to_word = {ix: word for ix, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:25.895351Z",
     "iopub.status.busy": "2023-08-07T11:06:25.895286Z",
     "iopub.status.idle": "2023-08-07T11:06:25.896833Z",
     "shell.execute_reply": "2023-08-07T11:06:25.896599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'opened': 0, 'fallen': 1, 'refuse': 2, 'impending': 3, 'bedroom': 4, 'portal': 5, 'has': 6, 'attempt': 7, 'Queen': 8, 'meets': 9, 'smack': 10, 'discovers': 11, 'goblins': 12, 'realms': 13, 'return': 14, 'war.': 15, 'have': 16, 'impossible': 17, \"Kara's\": 18, 'the': 19, 'closet': 20, 'Selinda,': 21, 'creatures,': 22, 'war': 23, 'To': 24, 'fairy': 25, 'Kara': 26, 'put': 27, 'assigns': 28, 'Princess': 29, 'an': 30, 'skills.': 31, 'astonishment,': 32, 'to': 33, 'into': 34, 'a': 35, 'pesky': 36, 'falls': 37, 'for': 38, 'two': 39, 'middle': 40, 'portal,': 41, 'huge': 42, 'Fairy': 43, 'task:': 44, 'using': 45, 'roust': 46, 'her': 47, 'that': 48, 'through!': 49, 'drafted': 50, 'and': 51, 'she': 52, 'They': 53, 'in': 54, 'be': 55, 'of': 56, 'who': 57, 'as': 58, 'through': 59, 'diplomatic': 60, 'appoints': 61, 'In': 62, 'end': 63}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:25.897912Z",
     "iopub.status.busy": "2023-08-07T11:06:25.897851Z",
     "iopub.status.idle": "2023-08-07T11:06:25.899887Z",
     "shell.execute_reply": "2023-08-07T11:06:25.899676Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating our training data and context window\n",
    "\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "data = []\n",
    "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1], raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:25.901063Z",
     "iopub.status.busy": "2023-08-07T11:06:25.900990Z",
     "iopub.status.idle": "2023-08-07T11:06:25.902692Z",
     "shell.execute_reply": "2023-08-07T11:06:25.902452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['To', \"Kara's\", 'she', 'discovers'], 'astonishment,')\n",
      "([\"Kara's\", 'astonishment,', 'discovers', 'that'], 'she')\n",
      "(['astonishment,', 'she', 'that', 'a'], 'discovers')\n",
      "(['she', 'discovers', 'a', 'portal'], 'that')\n",
      "(['discovers', 'that', 'portal', 'has'], 'a')\n",
      "(['that', 'a', 'has', 'opened'], 'portal')\n",
      "(['a', 'portal', 'opened', 'in'], 'has')\n",
      "(['portal', 'has', 'in', 'her'], 'opened')\n",
      "(['has', 'opened', 'her', 'bedroom'], 'in')\n",
      "(['opened', 'in', 'bedroom', 'closet'], 'her')\n"
     ]
    }
   ],
   "source": [
    "# We have our [input, input, input, input, target]\n",
    "# based on the context window of +2 words -2 words\n",
    "# you can see how we're building words close to each other now\n",
    "print(*data[0:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Set Up\n",
    "# CBOW Architecture \n",
    "\n",
    "<img width=\"344\" alt=\"Screen Shot 2023-02-14 at 3 48 16 PM\" src=\"https://user-images.githubusercontent.com/3837836/218859716-495a0a6f-aed7-40aa-aba9-5c0f1949788c.png\">\n",
    "\n",
    "We have two layers in the CBOW implementation of Word2Vec: an input Embedding layer that maps each word to a space in the embedding dictionary, a hidden linear activation layer, and then the output layer that is the proportional probabilities [softmax](https://en.wikipedia.org/wiki/Softmax_function) of all the correct words given an input window. \n",
    "\n",
    "The critical part is the first part, creating the Embeddings lookup. \n",
    "\n",
    "First, we associate each word in the vocabulary with an index, aka `{'she': 0, 'middle': 1, 'put': 2`\n",
    "\n",
    "Then, what we want to do is create an embeddings table, or matrix, that we will multiply with these indices to map each one to its correct place in relation to the other indices via a table lookup, based on how many vectors you'd like to represent the word. \n",
    "\n",
    "There is a [really good explanation](https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work/305032#305032) of how these are generated: \n",
    "\n",
    "``` \n",
    "For a given word, you create a one-hot vector based on its index and multiply it by the embeddings matrix, effectively replicating a lookup. For instance, for the word \"soon\" the index is 4, and the one-hot vector is [0, 0, 0, 0, 1, 0, 0]. If you multiply this (1, 7) matrix by the (7, 2) embeddings matrix you get the desired two-dimensional embedding, which in this case is [2.2, 1.4].\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:25.903791Z",
     "iopub.status.busy": "2023-08-07T11:06:25.903706Z",
     "iopub.status.idle": "2023-08-07T11:06:25.906293Z",
     "shell.execute_reply": "2023-08-07T11:06:25.906106Z"
    }
   },
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, embedding_dim\n",
    "    ):  # we pass in vocab_size and embedding_dim as hyperparams\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        # out: 1 x embedding_dim\n",
    "        self.embeddings = nn.Embedding(\n",
    "            vocab_size, embedding_dim\n",
    "        )  # initialize an Embedding matrix based on our inputs\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "\n",
    "        # out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1, -1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.tensor([word_to_ix[word]])\n",
    "        # Embeddings lookup of a single word once the Embeddings layer has been optimized\n",
    "        return self.embeddings(word).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:25.907353Z",
     "iopub.status.busy": "2023-08-07T11:06:25.907278Z",
     "iopub.status.idle": "2023-08-07T11:06:25.911658Z",
     "shell.execute_reply": "2023-08-07T11:06:25.911506Z"
    }
   },
   "outputs": [],
   "source": [
    "# We initialize the model:\n",
    "\n",
    "model = CBOW(vocab_size, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:25.912761Z",
     "iopub.status.busy": "2023-08-07T11:06:25.912692Z",
     "iopub.status.idle": "2023-08-07T11:06:25.914223Z",
     "shell.execute_reply": "2023-08-07T11:06:25.914030Z"
    }
   },
   "outputs": [],
   "source": [
    "# then, we initialize the loss function\n",
    "# (aka how close our predicted word is to the actual word and how we want to minimize it using the optimizer)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:25.915268Z",
     "iopub.status.busy": "2023-08-07T11:06:25.915205Z",
     "iopub.status.idle": "2023-08-07T11:06:26.300830Z",
     "shell.execute_reply": "2023-08-07T11:06:26.300487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of epoch 0 | loss 336.137\n",
      "end of epoch 1 | loss 324.402\n",
      "end of epoch 2 | loss 313.337\n",
      "end of epoch 3 | loss 302.782\n",
      "end of epoch 4 | loss 292.611\n",
      "end of epoch 5 | loss 282.781\n",
      "end of epoch 6 | loss 273.231\n",
      "end of epoch 7 | loss 263.993\n",
      "end of epoch 8 | loss 254.978\n",
      "end of epoch 9 | loss 246.182\n",
      "end of epoch 10 | loss 237.563\n",
      "end of epoch 11 | loss 229.070\n",
      "end of epoch 12 | loss 220.699\n",
      "end of epoch 13 | loss 212.436\n",
      "end of epoch 14 | loss 204.293\n",
      "end of epoch 15 | loss 196.253\n",
      "end of epoch 16 | loss 188.313\n",
      "end of epoch 17 | loss 180.481\n",
      "end of epoch 18 | loss 172.724\n",
      "end of epoch 19 | loss 165.083\n",
      "end of epoch 20 | loss 157.594\n",
      "end of epoch 21 | loss 150.262\n",
      "end of epoch 22 | loss 143.088\n",
      "end of epoch 23 | loss 136.076\n",
      "end of epoch 24 | loss 129.240\n",
      "end of epoch 25 | loss 122.594\n",
      "end of epoch 26 | loss 116.136\n",
      "end of epoch 27 | loss 109.904\n",
      "end of epoch 28 | loss 103.904\n",
      "end of epoch 29 | loss 98.137\n",
      "end of epoch 30 | loss 92.612\n",
      "end of epoch 31 | loss 87.343\n",
      "end of epoch 32 | loss 82.323\n",
      "end of epoch 33 | loss 77.559\n",
      "end of epoch 34 | loss 73.059\n",
      "end of epoch 35 | loss 68.814\n",
      "end of epoch 36 | loss 64.824\n",
      "end of epoch 37 | loss 61.074\n",
      "end of epoch 38 | loss 57.579\n",
      "end of epoch 39 | loss 54.310\n",
      "end of epoch 40 | loss 51.269\n",
      "end of epoch 41 | loss 48.438\n",
      "end of epoch 42 | loss 45.802\n",
      "end of epoch 43 | loss 43.352\n",
      "end of epoch 44 | loss 41.075\n",
      "end of epoch 45 | loss 38.963\n",
      "end of epoch 46 | loss 37.000\n",
      "end of epoch 47 | loss 35.171\n",
      "end of epoch 48 | loss 33.470\n",
      "end of epoch 49 | loss 31.890\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# 50 to start with, no correct answer here\n",
    "for epoch in range(50):\n",
    "    # we start tracking how accurate our intial words are\n",
    "    total_loss = 0\n",
    "\n",
    "    # for the x, y in the training data:\n",
    "    for context, target in data:\n",
    "        context_vector = make_context_vector(context, word_to_ix)\n",
    "\n",
    "        # we look at loss\n",
    "        log_probs = model(context_vector)\n",
    "\n",
    "        # we compare the loss from what the actual word is related to the probaility of the words\n",
    "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
    "\n",
    "    # optimize at the end of each epoch\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log out some metrics to see if loss decreases\n",
    "    print(\"end of epoch {} | loss {:2.3f}\".format(epoch, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:26.302181Z",
     "iopub.status.busy": "2023-08-07T11:06:26.302112Z",
     "iopub.status.idle": "2023-08-07T11:06:26.303987Z",
     "shell.execute_reply": "2023-08-07T11:06:26.303776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now, let's test to see if the model predicts the correct word using our initial input\n",
    "context = [\"Kara\", \"falls\", \"the\", \"portal\"]\n",
    "context_vector = make_context_vector(context, word_to_ix)\n",
    "a = model(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:26.305219Z",
     "iopub.status.busy": "2023-08-07T11:06:26.305157Z",
     "iopub.status.idle": "2023-08-07T11:06:26.309598Z",
     "shell.execute_reply": "2023-08-07T11:06:26.309347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: To Kara's astonishment, she discovers that a portal has opened in her bedroom closet and two goblins have fallen through! They refuse to return to the fairy realms and be drafted for an impending war. In an attempt to roust the pesky creatures, Kara falls through the portal, smack into the middle of a huge war. Kara meets Queen Selinda, who appoints Kara as a Fairy Princess and assigns her an impossible task: to put an end to the war using her diplomatic skills.\n",
      "\n",
      "Context: ['Kara', 'falls', 'the', 'portal']\n",
      "\n",
      "Prediction: creatures,\n"
     ]
    }
   ],
   "source": [
    "print(f'Raw text: {\" \".join(raw_text)}\\n')\n",
    "print(f\"Context: {context}\\n\")\n",
    "print(f\"Prediction: {ix_to_word[torch.argmax(a[0]).item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:26.310801Z",
     "iopub.status.busy": "2023-08-07T11:06:26.310718Z",
     "iopub.status.idle": "2023-08-07T11:06:26.319264Z",
     "shell.execute_reply": "2023-08-07T11:06:26.319082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vectors for a sequence:\n",
      " tensor([[-1.3122e+00, -5.9373e-03,  5.9570e-01, -1.8307e-01,  8.8574e-01,\n",
      "          3.7438e-01, -2.0238e-01,  2.2714e+00,  7.4274e-01,  1.4457e+00,\n",
      "         -1.0922e-01, -7.2746e-01,  1.8884e+00,  1.5903e+00, -2.2890e-01,\n",
      "          2.7072e-01, -4.3012e-01, -8.8410e-01,  1.0167e+00,  3.9204e-01,\n",
      "         -1.4966e-02, -6.9684e-01,  1.2691e+00, -7.9018e-01, -4.1049e-01,\n",
      "          6.9828e-01, -5.7492e-01,  1.3252e+00,  1.6870e-01, -1.8804e+00,\n",
      "         -6.9079e-01, -1.3518e+00,  1.8567e+00, -1.0202e+00,  6.8851e-01,\n",
      "          1.0466e+00, -1.5274e+00,  6.7465e-01,  5.1905e-01, -4.9612e-01,\n",
      "          9.1026e-01, -7.6268e-01,  6.6666e-01,  2.2071e+00, -9.0811e-01,\n",
      "         -2.4226e-01, -1.0076e+00,  7.9627e-01,  7.2852e-01, -6.6621e-01,\n",
      "          6.2031e-01, -3.4981e-01, -3.1215e-02,  5.9000e-01, -5.2303e-01,\n",
      "         -1.1391e-01,  2.5255e+00,  5.1060e-01,  1.9733e+00,  3.3426e-01,\n",
      "          9.6696e-01, -1.7444e+00, -6.5390e-01, -3.9971e-01,  1.7169e-01,\n",
      "         -4.2007e-01, -1.2152e-01,  4.6199e-01, -3.6661e-01, -1.2402e+00,\n",
      "          2.1244e-01,  5.8129e-01, -1.8909e+00, -8.7083e-01,  2.3274e-01,\n",
      "          1.6975e+00, -1.9238e+00,  2.2621e-01, -4.5820e-01, -1.6192e+00,\n",
      "          4.3254e-01, -5.8251e-01,  9.8595e-01, -5.9809e-01, -2.1624e-01,\n",
      "          3.0438e-01,  2.1641e-01, -1.1491e+00, -9.8554e-01, -8.3048e-01,\n",
      "          3.2269e-02, -1.1100e+00,  1.6462e-01,  7.9219e-01,  5.1157e-01,\n",
      "          6.0179e-01,  1.1963e+00,  5.7213e-01,  5.6430e-01,  1.1795e+00],\n",
      "        [-6.0571e-01,  1.4897e-01,  2.1758e-01,  1.0194e+00,  1.1295e+00,\n",
      "         -3.4981e-01,  2.8805e-01, -6.2771e-02,  6.6661e-01,  2.2222e+00,\n",
      "         -8.6768e-02, -1.4421e+00,  8.3184e-01, -8.1272e-01,  6.4110e-02,\n",
      "         -6.0833e-01,  9.0306e-01, -7.4857e-01, -1.2471e+00,  2.6363e-02,\n",
      "          1.3764e+00, -4.7780e-01,  1.0732e+00,  2.2633e+00, -5.4558e-01,\n",
      "         -3.6559e-01, -1.0899e+00,  1.2085e+00, -1.7388e+00,  1.4698e-01,\n",
      "         -4.6095e-01,  9.8524e-01, -8.8926e-01,  7.6879e-02, -5.7039e-02,\n",
      "         -5.3751e-01,  6.9069e-01,  1.0405e+00, -4.3404e+00,  3.5982e-01,\n",
      "          1.3233e-01,  3.2075e-01, -1.6496e+00,  2.4203e-01, -5.4269e-01,\n",
      "          6.9499e-01, -2.6949e-01, -2.1305e-01,  3.4738e-01,  8.2534e-01,\n",
      "         -1.1299e+00,  1.5842e-01, -9.8113e-01,  1.6596e+00,  2.0171e+00,\n",
      "          6.1068e-01, -1.3623e+00,  3.9343e-01,  1.5189e+00,  6.5462e-01,\n",
      "         -2.2310e-01,  9.5487e-01, -3.7008e-01, -8.6773e-01,  6.0823e-01,\n",
      "         -4.7870e-01,  1.5119e+00, -1.1584e-01, -1.1575e+00, -2.7670e-01,\n",
      "          6.1160e-02, -1.1903e-01, -6.1335e-01,  1.7833e-01,  5.9509e-01,\n",
      "         -1.2979e-01,  1.5060e+00, -8.6001e-01,  7.4784e-01, -1.1003e+00,\n",
      "         -1.3460e+00, -2.2041e-01,  5.6723e-02,  5.4733e-01, -1.1359e+00,\n",
      "         -3.7666e-01,  1.1228e-01,  4.9709e-01,  6.5980e-01,  1.4533e+00,\n",
      "         -4.2065e-01, -4.6526e-01,  6.9720e-01,  1.2335e-01, -1.9113e+00,\n",
      "         -1.1758e+00, -1.7196e-01,  9.6492e-01, -4.7383e-01,  1.6112e+00],\n",
      "        [-1.6724e+00,  3.2482e-01,  1.8194e-01, -6.4083e-01,  4.2445e-01,\n",
      "         -1.5549e+00,  9.0592e-01, -2.2628e+00, -8.2921e-01,  1.4348e+00,\n",
      "         -5.8504e-01,  1.1370e+00, -1.3998e+00, -1.2171e+00, -1.4116e+00,\n",
      "          3.1465e+00, -1.6446e-01, -7.6187e-01, -1.3043e+00,  8.1538e-01,\n",
      "          1.1332e+00,  4.1406e-01, -4.3832e-01, -9.4948e-04,  1.3787e+00,\n",
      "         -1.9562e-01,  7.8547e-01, -2.9748e-01, -1.3779e-01, -7.5809e-01,\n",
      "          1.4062e-01,  1.0379e+00, -2.5170e-01,  7.3235e-02, -6.0094e-01,\n",
      "         -1.1159e+00,  4.1990e-01,  6.4444e-01, -2.0312e-01, -1.5171e+00,\n",
      "         -2.0393e-01, -2.4747e+00,  7.9461e-01, -4.6139e-01,  1.1004e+00,\n",
      "          1.6911e+00,  3.1508e-01, -7.9413e-01,  2.0503e-01,  1.3970e-01,\n",
      "         -1.6554e+00, -6.3152e-03, -7.1200e-01,  1.2042e+00, -1.4692e+00,\n",
      "         -2.5790e-02,  4.1416e-02,  5.4367e-01,  4.1048e-01,  1.7774e+00,\n",
      "         -1.0575e+00, -1.3831e+00, -1.0596e+00, -8.0673e-01,  1.0410e+00,\n",
      "         -8.2401e-03,  3.2885e-01, -3.7881e-01, -3.6198e-01,  2.0824e-01,\n",
      "          1.1040e+00, -7.0521e-01,  8.7580e-01,  4.0711e-01,  3.8667e-01,\n",
      "         -3.7071e-01, -6.1365e-01,  2.3915e-01,  1.1462e-01, -9.0946e-02,\n",
      "          2.1410e+00, -2.0716e-01, -1.2745e+00,  4.2825e-01,  4.4593e-01,\n",
      "         -2.1550e-01,  9.2571e-01, -2.4776e-01, -6.6021e-01, -6.4505e-01,\n",
      "          4.9188e-01, -6.9300e-01, -2.6520e-01,  2.0365e+00,  6.9506e-01,\n",
      "          2.5417e+00, -9.1928e-01, -1.4073e+00,  5.1690e-01,  1.6990e+00]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now let's get what we care about, which is the embeddings!\n",
    "print(f\"Getting vectors for a sequence:\\n\", model.embeddings(torch.LongTensor([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:26.320330Z",
     "iopub.status.busy": "2023-08-07T11:06:26.320256Z",
     "iopub.status.idle": "2023-08-07T11:06:26.322729Z",
     "shell.execute_reply": "2023-08-07T11:06:26.322542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weights:\n",
      " tensor([-1.3122, -0.0059,  0.5957, -0.1831,  0.8857,  0.3744, -0.2024,  2.2714,\n",
      "         0.7427,  1.4457, -0.1092, -0.7275,  1.8884,  1.5903, -0.2289,  0.2707,\n",
      "        -0.4301, -0.8841,  1.0167,  0.3920, -0.0150, -0.6968,  1.2691, -0.7902,\n",
      "        -0.4105,  0.6983, -0.5749,  1.3252,  0.1687, -1.8804, -0.6908, -1.3518,\n",
      "         1.8567, -1.0202,  0.6885,  1.0466, -1.5274,  0.6746,  0.5190, -0.4961,\n",
      "         0.9103, -0.7627,  0.6667,  2.2071, -0.9081, -0.2423, -1.0076,  0.7963,\n",
      "         0.7285, -0.6662,  0.6203, -0.3498, -0.0312,  0.5900, -0.5230, -0.1139,\n",
      "         2.5255,  0.5106,  1.9733,  0.3343,  0.9670, -1.7444, -0.6539, -0.3997,\n",
      "         0.1717, -0.4201, -0.1215,  0.4620, -0.3666, -1.2402,  0.2124,  0.5813,\n",
      "        -1.8909, -0.8708,  0.2327,  1.6975, -1.9238,  0.2262, -0.4582, -1.6192,\n",
      "         0.4325, -0.5825,  0.9860, -0.5981, -0.2162,  0.3044,  0.2164, -1.1491,\n",
      "        -0.9855, -0.8305,  0.0323, -1.1100,  0.1646,  0.7922,  0.5116,  0.6018,\n",
      "         1.1963,  0.5721,  0.5643,  1.1795])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Getting weights:\\n\", model.embeddings.weight.data[1]\n",
    ")  # we can get the entire matrix this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:26.323760Z",
     "iopub.status.busy": "2023-08-07T11:06:26.323702Z",
     "iopub.status.idle": "2023-08-07T11:06:26.325691Z",
     "shell.execute_reply": "2023-08-07T11:06:26.325499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for Kara: tensor([ 1.0888, -1.1063,  0.8696, -0.3632, -0.9006,  0.7940, -1.1138,  2.1278,\n",
      "        -0.1689, -0.3420, -1.3995,  0.0601, -2.9023, -0.5998,  1.1930, -0.7473,\n",
      "         1.5664,  1.1403,  0.3928,  0.3714,  0.3632,  0.1330, -1.7100,  0.0202,\n",
      "         2.0143, -0.3630,  1.2034,  1.0377,  1.2923, -1.1941,  0.3272,  2.8864,\n",
      "        -1.4247,  0.4827,  1.4733, -1.1051,  0.7728, -0.3916, -0.2147, -0.2876,\n",
      "        -0.9865,  0.1281, -1.9323, -0.9826, -1.2451, -1.1102, -1.0630,  0.6419,\n",
      "        -1.5088,  1.0152,  0.4989, -0.0460,  0.2530, -0.2517, -1.1437, -0.3275,\n",
      "        -1.1290,  1.2488,  0.9396, -0.6129, -1.5827,  0.0347,  1.4194,  0.1029,\n",
      "        -0.8398, -1.2183, -1.9156, -0.1418, -0.5877, -0.8353, -2.2197, -1.4046,\n",
      "         1.4064,  0.6054,  0.7366,  0.9687, -0.4782, -1.3862,  0.2172, -0.6948,\n",
      "         1.0114,  0.0751, -1.7392,  1.6850,  1.2450, -0.6311,  0.3511, -0.5585,\n",
      "         0.3689, -0.5361,  1.6833, -1.1443, -0.5681, -0.4838,  0.2609, -1.9220,\n",
      "         0.4201,  0.5186,  1.3476,  0.4999], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# And, what we actually care about is being able to look up individual words with their embeddings:\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "print(f\"Embedding for Kara: {model.embeddings.weight[word_to_ix['Kara']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T11:06:26.326752Z",
     "iopub.status.busy": "2023-08-07T11:06:26.326693Z",
     "iopub.status.idle": "2023-08-07T11:06:26.328049Z",
     "shell.execute_reply": "2023-08-07T11:06:26.327867Z"
    }
   },
   "outputs": [],
   "source": [
    "# This way, when we create our second tower of book words, we know which ones are likely related to a given book"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "7983be674d93518c54b39475eb68739ef6a55aa8f4ec8a69dd7da1e80860d970"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
