{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9ec1aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -W ignore:DEPRECATION -m pip install --quiet duckdb==0.7.1 \\\n",
    "duckdb-engine \\\n",
    "watermark \\\n",
    "jupysql \\\n",
    "sqlalchemy \\\n",
    "python-snappy \\\n",
    "pyarrow \\\n",
    "memray \\\n",
    "pandas \\\n",
    "ipywidgets  \\\n",
    "matplotlib \\\n",
    "gensim \\\n",
    "nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "886c568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import re\n",
    "import pandas as pd\n",
    "import shlex\n",
    "import string\n",
    "from time import time\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "363cae19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking local machine specs for model processing\n",
    "cores = multiprocessing.cpu_count()\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4fe89c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    }
   ],
   "source": [
    "# set log level for model training\n",
    "import logging \n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dcb6ecb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging: 0.5.1.2\n",
      "gensim : 4.3.1\n",
      "pandas : 1.5.3\n",
      "re     : 2.2.1\n",
      "duckdb : 0.7.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "# Duckdb 0.7.0 offers a bunch of new JSON stuff that I want to test out, checking to see I have the latest\n",
    "# https://duckdb.org/2023/03/03/json.html\n",
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a7231357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7b07305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0c2381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DB or load existing https://duckdb.org/docs/guides/python/jupyter.html\n",
    "%sql duckdb:///viberary.duckdb\n",
    "    \n",
    "# connect with pyscopg\n",
    "con = duckdb.connect('viberary.duckdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd4954a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>column_type</th>\n",
       "      <th>null</th>\n",
       "      <th>key</th>\n",
       "      <th>default</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>isbn</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_reviews_count</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>series</td>\n",
       "      <td>BIGINT[]</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>country_code</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>language_code</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>popular_shelves</td>\n",
       "      <td>STRUCT(count BIGINT, \"name\" VARCHAR)[]</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>asin</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is_ebook</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>average_rating</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kindle_asin</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>similar_books</td>\n",
       "      <td>BIGINT[]</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>description</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>format</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>link</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>authors</td>\n",
       "      <td>STRUCT(author_id BIGINT, \"role\" VARCHAR)[]</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>publisher</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>num_pages</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>publication_day</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>isbn13</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>publication_month</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>edition_information</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>publication_year</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>url</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>image_url</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>book_id</td>\n",
       "      <td>BIGINT</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ratings_count</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>work_id</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>title</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>title_without_series</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             column_name                                 column_type null  \\\n",
       "0                   isbn                                     VARCHAR  YES   \n",
       "1     text_reviews_count                                     VARCHAR  YES   \n",
       "2                 series                                    BIGINT[]  YES   \n",
       "3           country_code                                     VARCHAR  YES   \n",
       "4          language_code                                     VARCHAR  YES   \n",
       "5        popular_shelves      STRUCT(count BIGINT, \"name\" VARCHAR)[]  YES   \n",
       "6                   asin                                     VARCHAR  YES   \n",
       "7               is_ebook                                     VARCHAR  YES   \n",
       "8         average_rating                                     VARCHAR  YES   \n",
       "9            kindle_asin                                     VARCHAR  YES   \n",
       "10         similar_books                                    BIGINT[]  YES   \n",
       "11           description                                     VARCHAR  YES   \n",
       "12                format                                     VARCHAR  YES   \n",
       "13                  link                                     VARCHAR  YES   \n",
       "14               authors  STRUCT(author_id BIGINT, \"role\" VARCHAR)[]  YES   \n",
       "15             publisher                                     VARCHAR  YES   \n",
       "16             num_pages                                     VARCHAR  YES   \n",
       "17       publication_day                                     VARCHAR  YES   \n",
       "18                isbn13                                     VARCHAR  YES   \n",
       "19     publication_month                                     VARCHAR  YES   \n",
       "20   edition_information                                     VARCHAR  YES   \n",
       "21      publication_year                                     VARCHAR  YES   \n",
       "22                   url                                     VARCHAR  YES   \n",
       "23             image_url                                     VARCHAR  YES   \n",
       "24               book_id                                      BIGINT  YES   \n",
       "25         ratings_count                                     VARCHAR  YES   \n",
       "26               work_id                                     VARCHAR  YES   \n",
       "27                 title                                     VARCHAR  YES   \n",
       "28  title_without_series                                     VARCHAR  YES   \n",
       "\n",
       "     key default extra  \n",
       "0   None    None  None  \n",
       "1   None    None  None  \n",
       "2   None    None  None  \n",
       "3   None    None  None  \n",
       "4   None    None  None  \n",
       "5   None    None  None  \n",
       "6   None    None  None  \n",
       "7   None    None  None  \n",
       "8   None    None  None  \n",
       "9   None    None  None  \n",
       "10  None    None  None  \n",
       "11  None    None  None  \n",
       "12  None    None  None  \n",
       "13  None    None  None  \n",
       "14  None    None  None  \n",
       "15  None    None  None  \n",
       "16  None    None  None  \n",
       "17  None    None  None  \n",
       "18  None    None  None  \n",
       "19  None    None  None  \n",
       "20  None    None  None  \n",
       "21  None    None  None  \n",
       "22  None    None  None  \n",
       "23  None    None  None  \n",
       "24  None    None  None  \n",
       "25  None    None  None  \n",
       "26  None    None  None  \n",
       "27  None    None  None  \n",
       "28  None    None  None  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DESCRIBE select * from read_json_auto('/Users/vicki/viberary/viberary/data/goodreads_books.json',lines='true');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1bb71c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(duckdb.CatalogException) Catalog Error: Table with name \"goodreads\" already exists!\n",
      "[SQL: CREATE TABLE goodreads as select * from read_json_auto('/Users/vicki/viberary/viberary/data/goodreads_books.json',lines='true');]\n",
      "(Background on this error at: https://sqlalche.me/e/14/f405)\n"
     ]
    }
   ],
   "source": [
    "# Create table in DuckDB\n",
    "%sql CREATE TABLE goodreads as select * from read_json_auto('/Users/vicki/viberary/viberary/data/goodreads_books.json',lines='true');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b883014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>title</th>\n",
       "      <th>ps</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5333265</td>\n",
       "      <td>W.C. Fields: A Life on Film</td>\n",
       "      <td>[{'count': 3, 'name': 'to-read'}, {'count': 1,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1333909</td>\n",
       "      <td>Good Harbor</td>\n",
       "      <td>[{'count': 2634, 'name': 'to-read'}, {'count':...</td>\n",
       "      <td>Anita Diamant's international bestseller \"The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7327624</td>\n",
       "      <td>The Unschooled Wizard (Sun Wolf and Starhawk, ...</td>\n",
       "      <td>[{'count': 58, 'name': 'to-read'}, {'count': 1...</td>\n",
       "      <td>Omnibus book club edition containing the Ladie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6066819</td>\n",
       "      <td>Best Friends Forever</td>\n",
       "      <td>[{'count': 7615, 'name': 'to-read'}, {'count':...</td>\n",
       "      <td>Addie Downs and Valerie Adler were eight when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>287140</td>\n",
       "      <td>Runic Astrology: Starcraft and Timekeeping in ...</td>\n",
       "      <td>[{'count': 32, 'name': 'to-read'}, {'count': 3...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>287141</td>\n",
       "      <td>The Aeneid for Boys and Girls</td>\n",
       "      <td>[{'count': 56, 'name': 'to-read'}, {'count': 1...</td>\n",
       "      <td>Relates in vigorous prose the tale of Aeneas, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>378460</td>\n",
       "      <td>The Wanting of Levine</td>\n",
       "      <td>[{'count': 14, 'name': 'to-read'}, {'count': 1...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6066812</td>\n",
       "      <td>All's Fairy in Love and War (Avalon: Web of Ma...</td>\n",
       "      <td>[{'count': 515, 'name': 'to-read'}, {'count': ...</td>\n",
       "      <td>To Kara's astonishment, she discovers that a p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34883016</td>\n",
       "      <td>Playmaker: A Venom Series Novella</td>\n",
       "      <td>[{'count': 4, 'name': 'to-read'}, {'count': 1,...</td>\n",
       "      <td>Secrets. Sometimes keeping them in confidence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>287149</td>\n",
       "      <td>The Devil's Notebook</td>\n",
       "      <td>[{'count': 961, 'name': 'to-read'}, {'count': ...</td>\n",
       "      <td>Wisdom, humor, and dark observations by the fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    book_id                                              title  \\\n",
       "0   5333265                        W.C. Fields: A Life on Film   \n",
       "1   1333909                                        Good Harbor   \n",
       "2   7327624  The Unschooled Wizard (Sun Wolf and Starhawk, ...   \n",
       "3   6066819                               Best Friends Forever   \n",
       "4    287140  Runic Astrology: Starcraft and Timekeeping in ...   \n",
       "5    287141                      The Aeneid for Boys and Girls   \n",
       "6    378460                              The Wanting of Levine   \n",
       "7   6066812  All's Fairy in Love and War (Avalon: Web of Ma...   \n",
       "8  34883016                  Playmaker: A Venom Series Novella   \n",
       "9    287149                               The Devil's Notebook   \n",
       "\n",
       "                                                  ps  \\\n",
       "0  [{'count': 3, 'name': 'to-read'}, {'count': 1,...   \n",
       "1  [{'count': 2634, 'name': 'to-read'}, {'count':...   \n",
       "2  [{'count': 58, 'name': 'to-read'}, {'count': 1...   \n",
       "3  [{'count': 7615, 'name': 'to-read'}, {'count':...   \n",
       "4  [{'count': 32, 'name': 'to-read'}, {'count': 3...   \n",
       "5  [{'count': 56, 'name': 'to-read'}, {'count': 1...   \n",
       "6  [{'count': 14, 'name': 'to-read'}, {'count': 1...   \n",
       "7  [{'count': 515, 'name': 'to-read'}, {'count': ...   \n",
       "8  [{'count': 4, 'name': 'to-read'}, {'count': 1,...   \n",
       "9  [{'count': 961, 'name': 'to-read'}, {'count': ...   \n",
       "\n",
       "                                         description  \n",
       "0                                                     \n",
       "1  Anita Diamant's international bestseller \"The ...  \n",
       "2  Omnibus book club edition containing the Ladie...  \n",
       "3  Addie Downs and Valerie Adler were eight when ...  \n",
       "4                                                     \n",
       "5  Relates in vigorous prose the tale of Aeneas, ...  \n",
       "6                                                     \n",
       "7  To Kara's astonishment, she discovers that a p...  \n",
       "8  Secrets. Sometimes keeping them in confidence ...  \n",
       "9  Wisdom, humor, and dark observations by the fo...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql select book_id, title, popular_shelves as ps, description from goodreads limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d53606a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>count_star()</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>2360165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_code  count_star()\n",
       "0           US       2360165\n",
       "1                        490"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql select country_code, count(*) from goodreads group by country_code;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfcf6fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language_code</th>\n",
       "      <th>count_star()</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ger</td>\n",
       "      <td>30941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>1060153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng</td>\n",
       "      <td>708457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en-US</td>\n",
       "      <td>91452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ara</td>\n",
       "      <td>42978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>tgk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>chn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>cop</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>sla</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>nub</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    language_code  count_star()\n",
       "0             ger         30941\n",
       "1                       1060153\n",
       "2             eng        708457\n",
       "3           en-US         91452\n",
       "4             ara         42978\n",
       "..            ...           ...\n",
       "222           tgk             1\n",
       "223           chn             1\n",
       "224           cop             1\n",
       "225           sla             1\n",
       "226           nub             1\n",
       "\n",
       "[227 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our results in Word2Vec initially are not so great, can we filter? \n",
    "%sql select language_code, count(*) from goodreads group by language_code;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16499cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language_code</th>\n",
       "      <th>count_star()</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng</td>\n",
       "      <td>708457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en-US</td>\n",
       "      <td>91452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en-GB</td>\n",
       "      <td>58358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en-CA</td>\n",
       "      <td>7652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>enm</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en-IN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language_code  count_star()\n",
       "0           eng        708457\n",
       "1         en-US         91452\n",
       "2         en-GB         58358\n",
       "3         en-CA          7652\n",
       "4            en           225\n",
       "5           enm            37\n",
       "6         en-IN             2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql select language_code, count(*) FROM goodreads \\\n",
    "WHERE language_code like 'en%' \\\n",
    "GROUP BY language_code;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f300006d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3256b48e029b4aafb6f13487c4d1f257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>866183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Count\n",
       "0  866183"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table that filters for english only for accuracy\n",
    "%sql CREATE TABLE goodreads_en as select * from goodreads WHERE language_code like 'en%';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "806947ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_rows</th>\n",
       "      <th>nulls</th>\n",
       "      <th>(count_star() FILTER (WHERE regexp_matches(description, ' ')) / CAST(count_star() AS FLOAT))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2360655</td>\n",
       "      <td>1945918</td>\n",
       "      <td>0.824313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_rows    nulls  \\\n",
       "0     2360655  1945918   \n",
       "\n",
       "   (count_star() FILTER (WHERE regexp_matches(description, ' ')) / CAST(count_star() AS FLOAT))  \n",
       "0                                           0.824313                                             "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percent of null descriptions (descriptions help accuracy)\n",
    "%sql select count(*) AS total_rows, \\\n",
    "count(*) FILTER (WHERE regexp_matches(description, ' ')) AS nulls ,\\\n",
    "count(*) FILTER (WHERE regexp_matches(description, ' ')) / count(*)::float \\\n",
    "FROM goodreads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6a0b9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301a68c0487e4e07b4b3f3804bf0b4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's start with title and description as our sentence features\n",
    "sentences = con.sql(\"\"\"select concat_ws(' ' , lower(regexp_replace(title, '[[:^alpha:]]',' ','g')), \\\n",
    "                    lower(regexp_replace(description, '[[:^alpha:]]',' ','g'))) as sentence from goodreads_en;\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb3f5e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the unschooled wizard  sun wolf and starhawk  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best friends forever addie downs and valerie a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the house of memory  pluto s snitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the bonfire of the vanities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heaven what is heaven really going to be like ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence\n",
       "0  the unschooled wizard  sun wolf and starhawk  ...\n",
       "1  best friends forever addie downs and valerie a...\n",
       "2           the house of memory  pluto s snitch     \n",
       "3                       the bonfire of the vanities \n",
       "4  heaven what is heaven really going to be like ..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each sentence is a single book\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f28c4eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for nulls\n",
    "sentences.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00bd38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.to_csv('sentences_en.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa113c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the unschooled wizard  sun wolf and starhawk        omnibus book club edition containing the ladies of madrigyn and the witches of wenshar \r\n",
      "best friends forever addie downs and valerie adler were eight when they first met and decided to be best friends forever  but  in the wake of tragedy and betrayal during their teenage years  everything changed  val went on to fame and fortune  addie stayed behind in their small midwestern town  destiny  however  had more in store for these two  and when  twenty five years later  val shows up at addie s front door with blood on her coat and terror on her face  it is the beginning of a wild adventure for two women joined by love and history who find strength together that they could not find alone \r\n",
      "the house of memory  pluto s snitch     \r\n",
      "the bonfire of the vanities \r\n",
      "heaven what is heaven really going to be like  what will we look like  what will we do  won t heaven get boring after a while  we all have questions about what heaven will be like  and after    years of extensive research  dr  randy alcorn has the answers  in the most comprehensive and definitive book on heaven to date  randy invites you to picture heaven the way scripture describes it   a bright  vibrant  and physical new earth  free from sin  suffering  and death  and brimming with christ s presence  wondrous natural beauty  and the richness of human culture as god intended it  god has put eternity in our hearts  now  randy alcorn brings eternity to light in a way that will surprise you  spark your imagination  and change how you live life today  if you ve always thought of heaven as a realm of disembodied spirits  clouds  and eternal harp strumming  you re in for a wonderful surprise  this is a book about real people with real bodies enjoying close relationships with god and each other  eating  drinking  working  playing  traveling  worshiping  and discovering on a new earth  earth as god created it  earth as he intended it to be  and the next time you hear someone say   we cant begin to imagine what heaven will be like   you ll be able to tell them   i can  \r\n",
      "dog heaven in newbery medalist cynthia rylant s classic bestseller  the author comforts readers young and old who have lost a dog  recommended highly by pet lovers around the world  dog heaven not only comforts but also brings a tear to anyone who is devoted to a pet  from expansive fields where dogs can run and run to delicious biscuits no dog can resist  rylant paints a warm and affectionate picture of the ideal place god would  of course  create for man s best friend  the first picture book illustrated by the author  dog heaven is enhanced by rylant s bright  bold paintings that perfectly capture an afterlife sure to bring solace to anyone who is grieving \r\n",
      "glimmering light \r\n",
      "the   s  fantastic films of the decades      \r\n",
      "crude world  the violent twilight of oil a stunning and revealing examination of oil s indelible impact on the countries that produce it and the people who possess it  every unhappy oil producing nation is unhappy in its own way  but all are touched by the  resource curse   the power of oil to exacerbate existing problems and create new ones  in crude world peter maass presents a vivid portrait of the troubled world oil has created  he takes us to saudi arabia  where officials deflect inquiries about the amount of petroleum remaining in the country s largest reservoir  to equatorial guinea  where two tennis courts grace an oil rich dictator s estate but bandages and aspirin are a hospital s only supplies  and to venezuela  where hugo chavez s campaign to redistribute oil wealth creates new economic and political crises  maass  a new york times magazinewriter  also introduces us to iraqi oilmen trying to rebuild their industry after the invasion of       an american lawyer leading ecuadorians in an unprecedented lawsuit against chevron  a russian oil billionaire imprisoned for his defiance of vladimir putin s leadership  and nigerian villagers whose livelihoods are destroyed by the discovery of oil  rebels  royalty  middlemen  environmentalists  indigenous activists  ceos  their stories  deftly and sensitively presented  tell the larger story of oil in our time  crude worldis a startling and essential account of the consequences of our addiction to oil \r\n",
      "untold secrets  fire   ice arrianna williams is an ordinary    yr  old woman or so she thinks  she has stumbled across a very special book and she soon sees just how special this book really is or at least she thinks she knows  the book comes to life and arrianna is no longer reading the pages but is in the story herself  shortly after reading the book she runs into an old childhood friend damian  she quickly falls for him but it doesn t last long  hurt  she finds comfort in another man  thinking her life is completely normal  she soon finds out she has a special gift that puts her life in danger  an archangel named gabriel comes to her rescue but in turn things only get worse  she is now being hunted for her gift and the man she sought comfort with is in desperate need of her to save his life  meanwhile her feelings for gabriel grow stronger as she wonders if he could be her true love  could this angel ever love her the way she loves him  will she defeat the fallen angel and demons after her  so many secrets so many lies and so much heartache will happen during her journey to discovering who she really is and what she was meant to do \r\n",
      "cat: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "! cat sentences_en.csv | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "71839618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input for training Word2Vec is a list of lists or iterable\n",
    "# needs to be streamable https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "class CorpusReader:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('sentences_en.csv')\n",
    "        for line in open(corpus_path):\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e52e1a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to where word2vec processes them\n",
    "!cp sentences_en.csv /usr/local/lib/python3.9/site-packages/gensim/test/test_data/sentences_en.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8241f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = CorpusReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d95f5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model = Word2Vec(min_count=20,\n",
    "#                      window=2,\n",
    "#                      vector_size=300,\n",
    "#                      sample=6e-5, \n",
    "#                      alpha=0.03, \n",
    "#                      min_alpha=0.0007, \n",
    "#                      negative=20,\n",
    "#                      workers=cores-1)\n",
    "\n",
    "# we can initialize this with our corpus but splitting out the steps makes them easier to see\n",
    "w2v_model = Word2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "652f0575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 1367451 words, keeping 52723 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 2719501 words, keeping 74831 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 4088179 words, keeping 92439 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 5440523 words, keeping 107566 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 6795839 words, keeping 121370 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 8163934 words, keeping 133976 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 9525001 words, keeping 145676 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 10911013 words, keeping 156238 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #90000, processed 12282965 words, keeping 166730 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #100000, processed 13657208 words, keeping 177248 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #110000, processed 15017317 words, keeping 186654 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #120000, processed 16388989 words, keeping 195741 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #130000, processed 17744206 words, keeping 204413 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #140000, processed 19095686 words, keeping 212823 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #150000, processed 20453943 words, keeping 220757 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #160000, processed 21810954 words, keeping 227947 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #170000, processed 23176332 words, keeping 235763 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #180000, processed 24506830 words, keeping 243344 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #190000, processed 25870673 words, keeping 250715 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #200000, processed 27255945 words, keeping 257923 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #210000, processed 28596806 words, keeping 264463 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #220000, processed 29972686 words, keeping 271050 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #230000, processed 31342018 words, keeping 277714 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #240000, processed 32696377 words, keeping 284269 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #250000, processed 34044708 words, keeping 290634 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #260000, processed 35396024 words, keeping 296657 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #270000, processed 36741584 words, keeping 302606 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #280000, processed 38086365 words, keeping 308219 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #290000, processed 39442909 words, keeping 314538 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #300000, processed 40800672 words, keeping 320315 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #310000, processed 42156970 words, keeping 326258 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #320000, processed 43512857 words, keeping 331959 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #330000, processed 44877145 words, keeping 337433 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #340000, processed 46230181 words, keeping 342690 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #350000, processed 47588852 words, keeping 348373 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #360000, processed 48956772 words, keeping 353688 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #370000, processed 50319552 words, keeping 358864 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #380000, processed 51674786 words, keeping 363807 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #390000, processed 53053632 words, keeping 369084 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #400000, processed 54421552 words, keeping 374038 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #410000, processed 55774295 words, keeping 378798 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #420000, processed 57118674 words, keeping 383362 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #430000, processed 58482543 words, keeping 388896 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #440000, processed 59852793 words, keeping 394087 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #450000, processed 61214994 words, keeping 398958 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #460000, processed 62576839 words, keeping 403491 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #470000, processed 63947551 words, keeping 407869 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #480000, processed 65280631 words, keeping 412402 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #490000, processed 66621549 words, keeping 417127 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #500000, processed 67991809 words, keeping 421481 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #510000, processed 69349984 words, keeping 425708 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #520000, processed 70718274 words, keeping 430007 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #530000, processed 72079601 words, keeping 434504 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #540000, processed 73446388 words, keeping 438737 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #550000, processed 74809861 words, keeping 443174 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #560000, processed 76179562 words, keeping 447469 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #570000, processed 77525721 words, keeping 451371 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #580000, processed 78900486 words, keeping 455443 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #590000, processed 80248106 words, keeping 459604 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #600000, processed 81608716 words, keeping 463757 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #610000, processed 82985132 words, keeping 467539 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #620000, processed 84348835 words, keeping 471406 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #630000, processed 85705966 words, keeping 475066 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #640000, processed 87057351 words, keeping 478880 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #650000, processed 88400910 words, keeping 482421 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #660000, processed 89752928 words, keeping 486118 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #670000, processed 91110673 words, keeping 490223 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #680000, processed 92466771 words, keeping 494046 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #690000, processed 93828408 words, keeping 498149 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #700000, processed 95174888 words, keeping 501856 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #710000, processed 96537390 words, keeping 505667 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #720000, processed 97884448 words, keeping 509550 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #730000, processed 99248037 words, keeping 512876 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #740000, processed 100609335 words, keeping 516470 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #750000, processed 101969547 words, keeping 520272 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #760000, processed 103323174 words, keeping 523785 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #770000, processed 104665690 words, keeping 527360 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #780000, processed 106017802 words, keeping 530611 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #790000, processed 107378178 words, keeping 534276 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #800000, processed 108736748 words, keeping 537926 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #810000, processed 110111745 words, keeping 541425 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #820000, processed 111471009 words, keeping 544887 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #830000, processed 112841366 words, keeping 548623 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #840000, processed 114204267 words, keeping 552305 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #850000, processed 115557962 words, keeping 555412 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #860000, processed 116916441 words, keeping 558576 word types\n",
      "INFO:gensim.models.word2vec:collected 560448 word types from a corpus of 117751998 raw words and 866183 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 158872 unique words (28.35% of original 560448, drops 401576)', 'datetime': '2023-03-25T12:44:46.145674', 'gensim': '4.3.1', 'python': '3.9.12 (main, Mar 26 2022, 15:51:13) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 117102677 word corpus (99.45% of original 117751998, drops 649321)', 'datetime': '2023-03-25T12:44:46.146330', 'gensim': '4.3.1', 'python': '3.9.12 (main, Mar 26 2022, 15:51:13) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 560448 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 39 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 92425613.47374666 word corpus (78.9%% of prior 117102677)', 'datetime': '2023-03-25T12:44:46.911453', 'gensim': '4.3.1', 'python': '3.9.12 (main, Mar 26 2022, 15:51:13) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 158872 words and 100 dimensions: 206533600 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-03-25T12:44:48.214478', 'gensim': '4.3.1', 'python': '3.9.12 (main, Mar 26 2022, 15:51:13) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 2.19 mins\n"
     ]
    }
   ],
   "source": [
    "# building vocab context window\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1f328bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 3 workers on 158872 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-03-25T12:47:25.381142', 'gensim': '4.3.1', 'python': '3.9.12 (main, Mar 26 2022, 15:51:13) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 0.52% examples, 475446 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 1.10% examples, 503613 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 1.66% examples, 507580 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 2.23% examples, 508803 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 2.76% examples, 503873 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 3.33% examples, 506740 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 3.91% examples, 510187 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 4.47% examples, 510897 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 5.01% examples, 507159 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 5.57% examples, 507722 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 6.13% examples, 508395 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 6.68% examples, 508335 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 7.25% examples, 508986 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 7.80% examples, 508493 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 8.38% examples, 510121 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 8.93% examples, 509901 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 9.45% examples, 508297 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 10.02% examples, 509289 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 10.57% examples, 509621 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 11.13% examples, 509709 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 11.70% examples, 510521 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 12.26% examples, 510006 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 12.84% examples, 510690 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 13.43% examples, 512133 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 13.93% examples, 510290 words/s, in_qsize 0, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 14.47% examples, 509010 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 15.01% examples, 508821 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 15.56% examples, 508717 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 16.07% examples, 507026 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 16.59% examples, 505813 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 17.14% examples, 505521 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 17.70% examples, 505605 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 18.23% examples, 504894 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 18.76% examples, 504287 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 19.32% examples, 503938 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 19.84% examples, 503300 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 20.42% examples, 503212 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 20.95% examples, 502253 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 21.48% examples, 502083 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 21.96% examples, 500591 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 22.54% examples, 500763 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 23.08% examples, 500755 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 23.63% examples, 500462 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 24.21% examples, 501146 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 24.74% examples, 500758 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 25.31% examples, 501306 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 25.91% examples, 502379 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 26.41% examples, 501513 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 26.97% examples, 501628 words/s, in_qsize 5, out_qsize 2\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 27.56% examples, 502586 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 28.13% examples, 502847 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 28.67% examples, 502717 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 29.24% examples, 503119 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 29.81% examples, 503265 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 30.37% examples, 503325 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 30.93% examples, 503436 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 31.49% examples, 503388 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 32.07% examples, 503885 words/s, in_qsize 2, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 32.62% examples, 503758 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 33.19% examples, 503977 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 33.69% examples, 503392 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 34.22% examples, 503193 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 34.79% examples, 503172 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 35.36% examples, 503515 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 35.86% examples, 502817 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 36.40% examples, 502732 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 36.96% examples, 502755 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 37.52% examples, 502885 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 38.07% examples, 502983 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 38.65% examples, 503169 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 39.20% examples, 503097 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 39.75% examples, 502892 words/s, in_qsize 4, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 40.31% examples, 503032 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 40.86% examples, 503030 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 41.41% examples, 503129 words/s, in_qsize 4, out_qsize 2\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 41.97% examples, 503312 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 42.55% examples, 503593 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 43.11% examples, 503791 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 43.68% examples, 503877 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 44.24% examples, 504161 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 44.81% examples, 504372 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 45.37% examples, 504537 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 45.93% examples, 504676 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 46.48% examples, 504726 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 47.04% examples, 504721 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 47.61% examples, 504915 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 48.19% examples, 505182 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 48.75% examples, 505237 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 49.30% examples, 505298 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 49.88% examples, 505531 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 50.45% examples, 505776 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 51.01% examples, 505787 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 51.56% examples, 505844 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 52.14% examples, 506102 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 52.71% examples, 506217 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 53.27% examples, 506306 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 53.84% examples, 506548 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 54.40% examples, 506595 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 54.98% examples, 506775 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 55.54% examples, 506652 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 56.11% examples, 506721 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 56.66% examples, 506680 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 57.21% examples, 506620 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 57.76% examples, 506636 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 58.31% examples, 506590 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 58.88% examples, 506667 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 59.45% examples, 506900 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 60.03% examples, 507051 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 60.59% examples, 507228 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 61.16% examples, 507323 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 61.73% examples, 507335 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 62.29% examples, 507389 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 62.84% examples, 507348 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 63.39% examples, 507315 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 63.94% examples, 507330 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 64.48% examples, 507230 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 65.06% examples, 507453 words/s, in_qsize 3, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 65.62% examples, 507516 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 66.17% examples, 507416 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 66.72% examples, 507355 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 67.25% examples, 507309 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 67.81% examples, 507268 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 68.41% examples, 507448 words/s, in_qsize 2, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 68.92% examples, 507155 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 69.50% examples, 507298 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 70.00% examples, 506905 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 70.53% examples, 506792 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 71.02% examples, 506413 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 71.56% examples, 506324 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 72.14% examples, 506484 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 72.70% examples, 506504 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 73.29% examples, 506701 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 73.83% examples, 506499 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 74.38% examples, 506452 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 74.93% examples, 506371 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 75.48% examples, 506348 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 76.00% examples, 506142 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 76.54% examples, 506050 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 77.10% examples, 506061 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 77.64% examples, 506035 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 78.16% examples, 505846 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 78.71% examples, 505793 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 79.28% examples, 505814 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 79.81% examples, 505679 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 80.32% examples, 505378 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 80.86% examples, 505289 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 81.39% examples, 505152 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 81.95% examples, 505186 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 82.52% examples, 505235 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 83.08% examples, 505279 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 83.64% examples, 505369 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 84.21% examples, 505393 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 84.76% examples, 505431 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 85.33% examples, 505514 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 85.89% examples, 505568 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 86.44% examples, 505539 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 87.01% examples, 505617 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 87.59% examples, 505732 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 88.17% examples, 505813 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 88.70% examples, 505678 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 89.27% examples, 505751 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 89.80% examples, 505595 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 90.36% examples, 505552 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 90.90% examples, 505370 words/s, in_qsize 5, out_qsize 2\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 91.46% examples, 505412 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 92.04% examples, 505579 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 92.57% examples, 505446 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 93.13% examples, 505517 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 93.68% examples, 505559 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 94.25% examples, 505646 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 94.79% examples, 505567 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 95.33% examples, 505533 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 95.88% examples, 505528 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 96.44% examples, 505593 words/s, in_qsize 2, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 97.00% examples, 505638 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 97.55% examples, 505662 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 98.11% examples, 505662 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 98.63% examples, 505454 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 99.21% examples, 505574 words/s, in_qsize 1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 99.78% examples, 505629 words/s, in_qsize 4, out_qsize 1\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 11896 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 3987 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 3978 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 3931 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 117751998 raw words (92423904 effective words) took 182.7s, 505857 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 0.51% examples, 473166 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 1.07% examples, 491763 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 1.64% examples, 503641 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 2.20% examples, 505608 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 2.78% examples, 510057 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 3.34% examples, 510825 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 3.90% examples, 512205 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 4.46% examples, 511229 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 5.02% examples, 510806 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 5.57% examples, 509684 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 6.12% examples, 509758 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 6.66% examples, 508557 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 7.19% examples, 505900 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 7.72% examples, 504279 words/s, in_qsize 5, out_qsize 2\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 8.30% examples, 505717 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 8.84% examples, 505862 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 9.36% examples, 504212 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 9.91% examples, 504553 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 10.48% examples, 505287 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 11.02% examples, 505504 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 11.58% examples, 506075 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 12.16% examples, 507049 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 12.71% examples, 507249 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 13.27% examples, 507285 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 13.83% examples, 507741 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 14.39% examples, 507705 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 14.93% examples, 507385 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 15.48% examples, 507202 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 16.03% examples, 507067 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 16.60% examples, 507385 words/s, in_qsize 0, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 17.12% examples, 506406 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 17.67% examples, 506292 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 18.21% examples, 505896 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 18.76% examples, 505740 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 19.28% examples, 504779 words/s, in_qsize 3, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 19.80% examples, 503830 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 20.36% examples, 503560 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 20.96% examples, 504460 words/s, in_qsize 3, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 21.52% examples, 504759 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 22.09% examples, 505030 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 22.63% examples, 504770 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 23.14% examples, 504199 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 23.69% examples, 503696 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 24.20% examples, 502738 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 24.73% examples, 502606 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 25.27% examples, 502401 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 25.80% examples, 502141 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 26.30% examples, 501240 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 26.87% examples, 501291 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 27.40% examples, 501087 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 27.96% examples, 501265 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 28.50% examples, 500839 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 29.03% examples, 500442 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 29.60% examples, 500415 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 30.15% examples, 500630 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 30.73% examples, 501049 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 31.29% examples, 500888 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 31.86% examples, 500959 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 32.41% examples, 500927 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 32.97% examples, 501028 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 33.52% examples, 501128 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 34.09% examples, 501507 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 34.65% examples, 501543 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 35.21% examples, 501490 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 35.77% examples, 501701 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 36.32% examples, 501624 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 36.88% examples, 501768 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 37.43% examples, 501773 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 37.97% examples, 501647 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 38.54% examples, 501964 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 39.11% examples, 502144 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 39.64% examples, 501760 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 40.19% examples, 501898 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 40.72% examples, 501637 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 41.28% examples, 501916 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 41.80% examples, 501645 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 42.35% examples, 501629 words/s, in_qsize 4, out_qsize 2\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 42.92% examples, 501959 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 43.52% examples, 502374 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 44.09% examples, 502397 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 44.62% examples, 502426 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 45.15% examples, 502249 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 45.68% examples, 502089 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 46.21% examples, 502007 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 46.76% examples, 501984 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 47.33% examples, 502176 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 47.89% examples, 502167 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 48.44% examples, 502191 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 49.00% examples, 502116 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 49.50% examples, 501789 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 50.04% examples, 501834 words/s, in_qsize 0, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 50.58% examples, 501609 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 51.16% examples, 501978 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 51.74% examples, 502281 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 52.31% examples, 502600 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 52.86% examples, 502599 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 53.39% examples, 502510 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 53.94% examples, 502559 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 54.49% examples, 502498 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 55.06% examples, 502568 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 55.63% examples, 502622 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 56.19% examples, 502605 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 56.74% examples, 502546 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 57.31% examples, 502673 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 57.86% examples, 502789 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 58.44% examples, 502984 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 58.99% examples, 503040 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 59.58% examples, 503435 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 60.07% examples, 502859 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 60.62% examples, 502862 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 61.20% examples, 503156 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 61.72% examples, 502952 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 62.23% examples, 502633 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 62.76% examples, 502538 words/s, in_qsize 4, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 63.30% examples, 502402 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 63.87% examples, 502496 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 64.45% examples, 502718 words/s, in_qsize 2, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 64.95% examples, 502380 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 65.51% examples, 502389 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 66.08% examples, 502521 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 66.64% examples, 502736 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 67.19% examples, 502760 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 67.76% examples, 502869 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 68.35% examples, 503088 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 68.91% examples, 503214 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 69.48% examples, 503289 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 70.03% examples, 503338 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 70.59% examples, 503527 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 71.16% examples, 503624 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 71.73% examples, 503828 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 72.31% examples, 503943 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 72.85% examples, 503921 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 73.44% examples, 504140 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 74.01% examples, 504260 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 74.60% examples, 504369 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 75.18% examples, 504504 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 75.74% examples, 504569 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 76.25% examples, 504315 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 76.82% examples, 504425 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 77.40% examples, 504591 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 77.95% examples, 504625 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 78.50% examples, 504614 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 79.07% examples, 504760 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 79.65% examples, 504910 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 80.22% examples, 505004 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 80.80% examples, 505125 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 81.37% examples, 505228 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 81.93% examples, 505309 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 82.50% examples, 505388 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 83.09% examples, 505582 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 83.61% examples, 505366 words/s, in_qsize 5, out_qsize 2\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 84.17% examples, 505451 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 84.74% examples, 505468 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 85.31% examples, 505475 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 85.87% examples, 505584 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 86.43% examples, 505565 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 87.00% examples, 505626 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 87.56% examples, 505630 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 88.13% examples, 505672 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 88.71% examples, 505828 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 89.27% examples, 505897 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 89.82% examples, 505825 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 90.37% examples, 505861 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 90.96% examples, 505958 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 91.50% examples, 505918 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 92.08% examples, 506028 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 92.65% examples, 506081 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 93.20% examples, 506078 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 93.76% examples, 506207 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 94.36% examples, 506444 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 94.86% examples, 506220 words/s, in_qsize 4, out_qsize 2\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 95.40% examples, 506168 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 95.92% examples, 505976 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 96.48% examples, 505992 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 97.01% examples, 505934 words/s, in_qsize 0, out_qsize 2\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 97.56% examples, 505939 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 98.13% examples, 505954 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 98.69% examples, 505995 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 99.27% examples, 506155 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 99.85% examples, 506244 words/s, in_qsize 4, out_qsize 1\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 11896 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 3994 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 3955 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 3947 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 117751998 raw words (92421755 effective words) took 182.5s, 506509 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 235503996 raw words (184845659 effective words) took 365.2s, 506178 effective words/s', 'datetime': '2023-03-25T12:53:30.476476', 'gensim': '4.3.1', 'python': '3.9.12 (main, Mar 26 2022, 15:51:13) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 6.08 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(corpus, total_examples=w2v_model.corpus_count, epochs=2, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "17932e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname_or_handle': 'word2vec.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-03-25T12:53:39.210730', 'gensim': '4.3.1', 'python': '3.9.12 (main, Mar 26 2022, 15:51:13) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}\n",
      "INFO:gensim.utils:storing np array 'vectors' to word2vec.model.wv.vectors.npy\n",
      "INFO:gensim.utils:storing np array 'syn1neg' to word2vec.model.syn1neg.npy\n",
      "INFO:gensim.utils:not storing attribute cum_table\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': 'word2vec.model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "# Saving and checkpointing\n",
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4b758efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from word2vec.model\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': 'word2vec.model', 'mode': 'rb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "INFO:gensim.utils:loading vectors from word2vec.model.wv.vectors.npy with mmap=None\n",
      "INFO:gensim.utils:loading syn1neg from word2vec.model.syn1neg.npy with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname': 'word2vec.model', 'datetime': '2023-03-25T12:53:43.681457', 'gensim': '4.3.1', 'python': '3.9.12 (main, Mar 26 2022, 15:51:13) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'loaded'}\n",
      "DEBUG:gensim.utils:starting a new internal lifecycle event log for KeyedVectors\n",
      "INFO:gensim.utils:KeyedVectors lifecycle event {'fname_or_handle': 'vectors.kv', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-03-25T12:53:43.682243', 'gensim': '4.3.1', 'python': '3.9.12 (main, Mar 26 2022, 15:51:13) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}\n",
      "INFO:gensim.utils:storing np array 'vectors' to vectors.kv.vectors.npy\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': 'vectors.kv', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:saved vectors.kv\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(\"word2vec.model\")\n",
    "word_vectors = w2v_model.wv\n",
    "word_vectors.save('vectors.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4bd09736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('monster', 0.6978684663772583),\n",
       " ('wolf', 0.6856111288070679),\n",
       " ('lion', 0.6570420861244202),\n",
       " ('creature', 0.656029462814331),\n",
       " ('dragon', 0.6286692023277283),\n",
       " ('tiger', 0.6162548065185547),\n",
       " ('jaguar', 0.6116740107536316),\n",
       " ('tigress', 0.6096420884132385),\n",
       " ('demon', 0.6021472811698914),\n",
       " ('caged', 0.5965006351470947)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"beast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "eeb58f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elegance', 0.6451990008354187),\n",
       " ('allure', 0.6288026571273804),\n",
       " ('loveliness', 0.6085167527198792),\n",
       " ('gentleness', 0.6060893535614014),\n",
       " ('ravishing', 0.6009306907653809),\n",
       " ('sensuous', 0.5986630916595459),\n",
       " ('lass', 0.5856353640556335),\n",
       " ('charm', 0.5812059640884399),\n",
       " ('sensuality', 0.5809239149093628),\n",
       " ('lyricism', 0.5799146294593811)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"beauty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e4bcef5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beatrix', 0.7520222067832947),\n",
       " ('peter', 0.630151093006134),\n",
       " ('draco', 0.5766912698745728),\n",
       " ('dresden', 0.5766406059265137),\n",
       " ('hieronymus', 0.5762723684310913),\n",
       " ('tom', 0.5648345351219177),\n",
       " ('jack', 0.5640392899513245),\n",
       " ('charlie', 0.5528959035873413),\n",
       " ('drarry', 0.5491214394569397),\n",
       " ('snape', 0.5455459952354431)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now doing a semantic test for actual book characters\n",
    "w2v_model.wv.most_similar(\"harry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4573ca07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('updike', 0.8069586753845215),\n",
       " ('thurber', 0.7918877601623535),\n",
       " ('morrell', 0.7728648781776428),\n",
       " ('gould', 0.7717663049697876),\n",
       " ('piccirilli', 0.7693771719932556),\n",
       " ('hogg', 0.7692300081253052),\n",
       " ('bowles', 0.7571479082107544),\n",
       " ('banville', 0.7553629279136658),\n",
       " ('lethem', 0.7543212175369263),\n",
       " ('straub', 0.749229371547699)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"copperfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7edde8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19518006, -1.7672642 ,  0.5005682 ,  0.9529076 , -1.7250446 ,\n",
       "       -2.6468692 ,  1.9986058 , -0.46701965,  0.2481065 , -0.6591733 ,\n",
       "       -0.42162445,  1.3421237 ,  1.352153  ,  0.90025795,  0.43388948,\n",
       "       -1.8020738 ,  1.3958081 , -1.2582821 , -2.0085955 ,  0.22990757,\n",
       "        1.970394  ,  0.50289434,  1.8834186 , -0.38901907, -1.9855825 ,\n",
       "        0.08820678,  0.32120776,  1.2121825 ,  0.6935328 , -0.39396322,\n",
       "        2.7387917 , -0.18162552, -2.168393  ,  0.49018154, -0.88244426,\n",
       "       -0.88038564, -5.278482  , -0.6654004 , -0.27781636,  0.69942164,\n",
       "       -1.7041072 , -1.8432969 ,  0.8142995 , -0.15949471,  0.06960123,\n",
       "       -0.7476442 , -1.2020489 ,  0.5902427 ,  0.04238603, -0.4309581 ,\n",
       "        1.1691087 ,  0.18789019, -1.8825562 ,  1.7118117 , -0.25407398,\n",
       "        2.582912  ,  0.5353555 , -1.3065976 ,  0.8374112 ,  0.29024607,\n",
       "        1.3353986 , -3.106418  , -0.59596825, -0.7341918 , -1.0472411 ,\n",
       "        2.7758048 , -2.7797241 , -0.5299576 , -1.3266624 ,  0.9727999 ,\n",
       "        1.3956001 ,  1.0207385 ,  0.6948733 ,  1.7189895 , -0.64890164,\n",
       "       -1.0483829 ,  1.1432935 ,  0.9862758 ,  3.4369376 ,  2.16014   ,\n",
       "       -2.1515832 ,  2.094732  , -1.37834   , -1.3250194 ,  0.16120999,\n",
       "        1.5799396 , -1.0251545 ,  0.6490195 ,  0.30813694,  2.1682873 ,\n",
       "        0.13724527, -1.5376854 ,  0.759318  , -2.192834  ,  0.24334745,\n",
       "       -0.16672592, -0.19886778, -1.5678107 , -1.8081565 , -2.1753376 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's an individual embedding!\n",
    "vec_beauty = w2v_model.wv['beast']\n",
    "vec_beauty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c2e31b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158872"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len = len(w2v_model.wv)\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "515ec727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'and': 1,\n",
       " 'of': 2,\n",
       " 'to': 3,\n",
       " 'in': 4,\n",
       " 'is': 5,\n",
       " 'her': 6,\n",
       " 'his': 7,\n",
       " 'she': 8,\n",
       " 'for': 9,\n",
       " 'he': 10,\n",
       " 'with': 11,\n",
       " 'that': 12,\n",
       " 'as': 13,\n",
       " 'it': 14,\n",
       " 'but': 15,\n",
       " 'on': 16,\n",
       " 'from': 17,\n",
       " 'an': 18,\n",
       " 'has': 19}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the key-value mapping\n",
    "import itertools\n",
    "\n",
    "dict(itertools.islice(w2v_model.wv.key_to_index.items(), 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "178ff7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/158872 is the\n",
      "word #1/158872 is and\n",
      "word #2/158872 is of\n",
      "word #3/158872 is to\n",
      "word #4/158872 is in\n",
      "word #5/158872 is is\n",
      "word #6/158872 is her\n",
      "word #7/158872 is his\n",
      "word #8/158872 is she\n",
      "word #9/158872 is for\n"
     ]
    }
   ],
   "source": [
    "# enumerate the index/key of each word \n",
    "\n",
    "for index, word in enumerate(w2v_model.wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(w2v_model.wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd71efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 158872 samples in 0.006s...\n",
      "[t-SNE] Computed neighbors for 158872 samples in 36.561s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 19000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 20000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 21000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 22000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 23000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 24000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 25000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 26000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 27000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 28000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 29000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 30000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 31000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 32000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 33000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 34000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 35000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 36000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 37000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 38000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 39000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 40000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 41000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 42000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 43000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 44000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 45000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 46000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 47000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 48000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 49000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 50000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 51000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 52000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 53000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 54000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 55000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 56000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 57000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 58000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 59000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 60000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 61000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 62000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 63000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 64000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 65000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 66000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 67000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 68000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 69000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 70000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 71000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 72000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 73000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 74000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 75000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 76000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 77000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 78000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 79000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 80000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 81000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 82000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 83000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 84000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 85000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 86000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 87000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 88000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 89000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 90000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 91000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 92000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 93000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 94000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 95000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 96000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 97000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 98000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 99000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 100000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 101000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 102000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 103000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 104000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 105000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 106000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 107000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 108000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 109000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 110000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 111000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 112000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 113000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 114000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 115000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 116000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 117000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 118000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 119000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 120000 / 158872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computed conditional probabilities for sample 121000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 122000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 123000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 124000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 125000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 126000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 127000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 128000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 129000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 130000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 131000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 132000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 133000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 134000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 135000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 136000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 137000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 138000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 139000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 140000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 141000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 142000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 143000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 144000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 145000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 146000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 147000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 148000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 149000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 150000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 151000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 152000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 153000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 154000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 155000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 156000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 157000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 158000 / 158872\n",
      "[t-SNE] Computed conditional probabilities for sample 158872 / 158872\n",
      "[t-SNE] Mean sigma: 0.105353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    # extract the words & their vectors, as numpy arrays\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
    "\n",
    "    # reduce using t-SNE and log out\n",
    "    '''\n",
    "    Expected logs: \n",
    "    [t-SNE] Computing 91 nearest neighbors...\n",
    "    [t-SNE] Indexed 100 samples in 0.000s...\n",
    "    [t-SNE] Computed neighbors for 100 samples in 0.005s...\n",
    "    [t-SNE] Computed conditional probabilities for sample 100 / 100\n",
    "    [t-SNE] Mean sigma: 1.000000\n",
    "    [t-SNE] KL divergence after 250 iterations with early exaggeration: 55.965740\n",
    "    [t-SNE] KL divergence after 1000 iterations: 0.386509\n",
    "    '''\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0, verbose=2)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a1b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "except Exception:\n",
    "    plot_function = plot_with_matplotlib\n",
    "else:\n",
    "    plot_function = plot_with_plotly\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
